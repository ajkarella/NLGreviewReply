{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ambrose\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.5.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and tag\n",
    "\n",
    "reviews = []\n",
    "responses = []\n",
    "\n",
    "with open('review.dat', 'rb') as f:\n",
    "    rev = pickle.load(f)\n",
    "with open('reply.dat', 'rb') as f:\n",
    "    rep = pickle.load(f)\n",
    "    \n",
    "for i in range(len(rep)):\n",
    "    responses.append( '<START> ' + rep[i] + ' <END>' )\n",
    "    \n",
    "reviews = rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[0:int(len(reviews)/2)]\n",
    "responses = responses[0:int(len(responses)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( reviews + responses )\n",
    "\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "#print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list , vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 961) 961\n",
      "(536, 809) 809\n",
      "(536, 809, 8656)\n"
     ]
    }
   ],
   "source": [
    "# encoder_input_data\n",
    "tokenized_reviews = tokenizer.texts_to_sequences(reviews)\n",
    "maxlen_reviews = max([len(x) for x in tokenized_reviews])\n",
    "padded_reviews = preprocessing.sequence.pad_sequences(tokenized_reviews , maxlen=maxlen_reviews , padding='post')\n",
    "encoder_input_data = np.array(padded_reviews)\n",
    "#print(encoder_input_data.shape , maxlen_reviews)\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_responses = tokenizer.texts_to_sequences(responses)\n",
    "maxlen_responses = max([len(x) for x in tokenized_responses])\n",
    "padded_responses = preprocessing.sequence.pad_sequences(tokenized_responses , maxlen=maxlen_responses , padding='post')\n",
    "decoder_input_data = np.array(padded_responses)\n",
    "#print(decoder_input_data.shape , maxlen_responses)\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_responses = tokenizer.texts_to_sequences(responses)\n",
    "for i in range(len(tokenized_responses)) :\n",
    "    tokenized_responses[i] = tokenized_responses[i][1:]\n",
    "padded_responses = preprocessing.sequence.pad_sequences(tokenized_responses , maxlen=maxlen_responses , padding='post')\n",
    "onehot_responses = utils.to_categorical(padded_responses , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_responses)\n",
    "#print( decoder_output_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 961)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 809)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 961, 200)     1731200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 809, 200)     1731200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 809, 200), ( 320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 809, 8656)    1739856     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,843,856\n",
      "Trainable params: 5,843,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model creation\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_reviews , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)( encoder_embedding )\n",
    "encoder_states = [state_h , state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_responses ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "22/22 [==============================] - 19s 381ms/step - loss: 0.7917\n",
      "Epoch 2/150\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.7084\n",
      "Epoch 3/150\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 0.6996\n",
      "Epoch 4/150\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.6901\n",
      "Epoch 5/150\n",
      "22/22 [==============================] - 8s 380ms/step - loss: 0.6824\n",
      "Epoch 6/150\n",
      "22/22 [==============================] - 9s 390ms/step - loss: 0.6759\n",
      "Epoch 7/150\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.6690\n",
      "Epoch 8/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.6612\n",
      "Epoch 9/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.6548\n",
      "Epoch 10/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.6480\n",
      "Epoch 11/150\n",
      "22/22 [==============================] - 9s 413ms/step - loss: 0.6418\n",
      "Epoch 12/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.6398\n",
      "Epoch 13/150\n",
      "22/22 [==============================] - 9s 409ms/step - loss: 0.6283\n",
      "Epoch 14/150\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 0.6216\n",
      "Epoch 15/150\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 0.6142\n",
      "Epoch 16/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.6076\n",
      "Epoch 17/150\n",
      "22/22 [==============================] - 9s 419ms/step - loss: 0.6005\n",
      "Epoch 18/150\n",
      "22/22 [==============================] - 9s 412ms/step - loss: 0.5933\n",
      "Epoch 19/150\n",
      "22/22 [==============================] - 9s 409ms/step - loss: 0.5863\n",
      "Epoch 20/150\n",
      "22/22 [==============================] - 9s 413ms/step - loss: 0.5797\n",
      "Epoch 21/150\n",
      "22/22 [==============================] - 9s 414ms/step - loss: 0.5721\n",
      "Epoch 22/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.5656\n",
      "Epoch 23/150\n",
      "22/22 [==============================] - 9s 418ms/step - loss: 0.5579\n",
      "Epoch 24/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.5510\n",
      "Epoch 25/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.5440\n",
      "Epoch 26/150\n",
      "22/22 [==============================] - 9s 418ms/step - loss: 0.5374\n",
      "Epoch 27/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.5310\n",
      "Epoch 28/150\n",
      "22/22 [==============================] - 9s 412ms/step - loss: 0.5245\n",
      "Epoch 29/150\n",
      "22/22 [==============================] - 9s 426ms/step - loss: 0.5183\n",
      "Epoch 30/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.5121\n",
      "Epoch 31/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.5067\n",
      "Epoch 32/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.5007\n",
      "Epoch 33/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.4949\n",
      "Epoch 34/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.4891\n",
      "Epoch 35/150\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 0.4837\n",
      "Epoch 36/150\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.4784\n",
      "Epoch 37/150\n",
      "22/22 [==============================] - 9s 410ms/step - loss: 0.4734\n",
      "Epoch 38/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.4673\n",
      "Epoch 39/150\n",
      "22/22 [==============================] - 9s 415ms/step - loss: 0.4624\n",
      "Epoch 40/150\n",
      "22/22 [==============================] - 9s 413ms/step - loss: 0.4574\n",
      "Epoch 41/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.4520\n",
      "Epoch 42/150\n",
      "22/22 [==============================] - 9s 421ms/step - loss: 0.4468\n",
      "Epoch 43/150\n",
      "22/22 [==============================] - 9s 410ms/step - loss: 0.4422\n",
      "Epoch 44/150\n",
      "22/22 [==============================] - 9s 429ms/step - loss: 0.4369\n",
      "Epoch 45/150\n",
      "22/22 [==============================] - 9s 414ms/step - loss: 0.4320\n",
      "Epoch 46/150\n",
      "22/22 [==============================] - 9s 421ms/step - loss: 0.4274\n",
      "Epoch 47/150\n",
      "22/22 [==============================] - 9s 425ms/step - loss: 0.4220\n",
      "Epoch 48/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.4178\n",
      "Epoch 49/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.4126\n",
      "Epoch 50/150\n",
      "22/22 [==============================] - 9s 414ms/step - loss: 0.4084\n",
      "Epoch 51/150\n",
      "22/22 [==============================] - 9s 416ms/step - loss: 0.4034\n",
      "Epoch 52/150\n",
      "22/22 [==============================] - 9s 418ms/step - loss: 0.3992\n",
      "Epoch 53/150\n",
      "22/22 [==============================] - 9s 412ms/step - loss: 0.3942\n",
      "Epoch 54/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.3901\n",
      "Epoch 55/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.3851\n",
      "Epoch 56/150\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.3808\n",
      "Epoch 57/150\n",
      "22/22 [==============================] - 9s 414ms/step - loss: 0.3765\n",
      "Epoch 58/150\n",
      "22/22 [==============================] - 9s 411ms/step - loss: 0.3723\n",
      "Epoch 59/150\n",
      "22/22 [==============================] - 9s 414ms/step - loss: 0.3677\n",
      "Epoch 60/150\n",
      "22/22 [==============================] - 9s 417ms/step - loss: 0.3640\n",
      "Epoch 61/150\n",
      "22/22 [==============================] - 9s 416ms/step - loss: 0.3587\n",
      "Epoch 62/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.3552\n",
      "Epoch 63/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.3509\n",
      "Epoch 64/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.3466\n",
      "Epoch 65/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.3427\n",
      "Epoch 66/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.3382\n",
      "Epoch 67/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.3349\n",
      "Epoch 68/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.3309\n",
      "Epoch 69/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.3262\n",
      "Epoch 70/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.3230\n",
      "Epoch 71/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.3185\n",
      "Epoch 72/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.3148\n",
      "Epoch 73/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.3112\n",
      "Epoch 74/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.3072\n",
      "Epoch 75/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.3034\n",
      "Epoch 76/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.2994\n",
      "Epoch 77/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.2964\n",
      "Epoch 78/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.2925\n",
      "Epoch 79/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.2894\n",
      "Epoch 80/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.2856\n",
      "Epoch 81/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.2824\n",
      "Epoch 82/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.2786\n",
      "Epoch 83/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.2754\n",
      "Epoch 84/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.2722\n",
      "Epoch 85/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.2688\n",
      "Epoch 86/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.2657\n",
      "Epoch 87/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.2619\n",
      "Epoch 88/150\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.2596\n",
      "Epoch 89/150\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.2556\n",
      "Epoch 90/150\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.2525\n",
      "Epoch 91/150\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.2499\n",
      "Epoch 92/150\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.2468\n",
      "Epoch 93/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.2436\n",
      "Epoch 94/150\n",
      "22/22 [==============================] - 9s 393ms/step - loss: 0.2411\n",
      "Epoch 95/150\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.2374\n",
      "Epoch 96/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.2346\n",
      "Epoch 97/150\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.2323\n",
      "Epoch 98/150\n",
      "22/22 [==============================] - 9s 390ms/step - loss: 0.2294\n",
      "Epoch 99/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 9s 398ms/step - loss: 0.2267\n",
      "Epoch 100/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.2241\n",
      "Epoch 101/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.2206\n",
      "Epoch 102/150\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.2187\n",
      "Epoch 103/150\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.2154\n",
      "Epoch 104/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.2131\n",
      "Epoch 105/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.2107\n",
      "Epoch 106/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.2081\n",
      "Epoch 107/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.2050\n",
      "Epoch 108/150\n",
      "22/22 [==============================] - 10s 441ms/step - loss: 0.2024\n",
      "Epoch 109/150\n",
      "22/22 [==============================] - 10s 457ms/step - loss: 0.2007\n",
      "Epoch 110/150\n",
      "22/22 [==============================] - 9s 415ms/step - loss: 0.1969\n",
      "Epoch 111/150\n",
      "22/22 [==============================] - 9s 413ms/step - loss: 0.1953\n",
      "Epoch 112/150\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.1926\n",
      "Epoch 113/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.1903\n",
      "Epoch 114/150\n",
      "22/22 [==============================] - 9s 420ms/step - loss: 0.1875\n",
      "Epoch 115/150\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.1859\n",
      "Epoch 116/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.1843\n",
      "Epoch 117/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.1804\n",
      "Epoch 118/150\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.1785\n",
      "Epoch 119/150\n",
      "22/22 [==============================] - 9s 409ms/step - loss: 0.1761\n",
      "Epoch 120/150\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 0.1739\n",
      "Epoch 121/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.1730\n",
      "Epoch 122/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.1692\n",
      "Epoch 123/150\n",
      "22/22 [==============================] - 9s 385ms/step - loss: 0.1685\n",
      "Epoch 124/150\n",
      "22/22 [==============================] - 9s 423ms/step - loss: 0.1651\n",
      "Epoch 125/150\n",
      "22/22 [==============================] - 10s 434ms/step - loss: 0.1632\n",
      "Epoch 126/150\n",
      "22/22 [==============================] - 10s 449ms/step - loss: 0.1617\n",
      "Epoch 127/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.1603\n",
      "Epoch 128/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.1567\n",
      "Epoch 129/150\n",
      "22/22 [==============================] - 9s 413ms/step - loss: 0.1550\n",
      "Epoch 130/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.1534\n",
      "Epoch 131/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.1521\n",
      "Epoch 132/150\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 0.1492\n",
      "Epoch 133/150\n",
      "22/22 [==============================] - 9s 401ms/step - loss: 0.1482\n",
      "Epoch 134/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.1445\n",
      "Epoch 135/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.1444\n",
      "Epoch 136/150\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.1419\n",
      "Epoch 137/150\n",
      "22/22 [==============================] - 8s 386ms/step - loss: 0.1400\n",
      "Epoch 138/150\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.1375\n",
      "Epoch 139/150\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.1373\n",
      "Epoch 140/150\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 0.1343\n",
      "Epoch 141/150\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.1330\n",
      "Epoch 142/150\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 0.1313\n",
      "Epoch 143/150\n",
      "22/22 [==============================] - 9s 419ms/step - loss: 0.1291\n",
      "Epoch 144/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.1280\n",
      "Epoch 145/150\n",
      "22/22 [==============================] - 9s 422ms/step - loss: 0.1264\n",
      "Epoch 146/150\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.1246\n",
      "Epoch 147/150\n",
      "22/22 [==============================] - 9s 411ms/step - loss: 0.1220\n",
      "Epoch 148/150\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.1214\n",
      "Epoch 149/150\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.1192\n",
      "Epoch 150/150\n",
      "22/22 [==============================] - 8s 369ms/step - loss: 0.1177\n"
     ]
    }
   ],
   "source": [
    "#attempt larger batch size if comp can handle it (50)\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=25, epochs=150 ) \n",
    "\n",
    "# this method of saving should not be trusted\n",
    "# reloading this model breaks system... saving weights better\n",
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input review to encoder\n",
    "# input hidden and mem states, and replies to decoder\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(200,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(200,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input str to tokens\n",
    "def str_to_tokens(sentence : str):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append(tokenizer.word_index[word]) \n",
    "        except:\n",
    "            pass\n",
    "    return preprocessing.sequence.pad_sequences([tokens_list],maxlen=maxlen_reviews,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# reloading model because I shut down my computer\\n\\nimport os\\n\\nmodel = tf.keras.models.load_model('model.h5')\\nmodel.summary()\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this does not work. \n",
    "\n",
    "'''\n",
    "# reloading model because I shut down my computer\n",
    "\n",
    "import os\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eval metrics and get test set\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import nltk.translate.nist_score as nist\n",
    "\n",
    "test_reviews = rev[int(len(rev)/2):]\n",
    "test_responses = rep[int(len(rep)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through test set and compile lists for scores and \n",
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "nScore = []\n",
    "bScore = []\n",
    "translation = []\n",
    "\n",
    "for i in range(100):\n",
    "    states_values = enc_model.predict(str_to_tokens(test_reviews[i]))\n",
    "    empty_target_seq = np.zeros((1,1))\n",
    "    empty_target_seq[0,0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition:\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_responses:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1,1))  \n",
    "        empty_target_seq[0,0] = sampled_word_index\n",
    "        states_values = [h,c] \n",
    "    \n",
    "    translation.append(decoded_translation[1:-4])\n",
    "    bScore.append(bleu.sentence_bleu(test_responses[i][8:-4], decoded_translation[1:-4]))\n",
    "    nScore.append(nist.sentence_nist(test_responses[i][8:-4], decoded_translation[1:-4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n average: 0.05435444922513263\n",
      "n min: 0.00437513244752933\n",
      "n max: 0.34968416183086115\n",
      "\n",
      "b average: 1.0868379274593338e-231\n",
      "b min: 6.24411123256442e-232\n",
      "b max: 1.7296736363974997e-231\n"
     ]
    }
   ],
   "source": [
    "# print min max, and avgs\n",
    "print(\"n average:\", sum(nScore) / len(nScore))\n",
    "print(\"n min:\", min(nScore))\n",
    "print(\"n max:\", max(nScore))\n",
    "print()\n",
    "print(\"b average:\", sum(bScore) / len(bScore))\n",
    "print(\"b min:\", min(bScore))\n",
    "print(\"b max:\", max(bScore))\n",
    "\n",
    "## for ref, the hw model returned the following scores on the same dataset\n",
    "# avg: 0.000920\n",
    "# min: 0.000000\n",
    "# max: 0.091984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idx of min and max \n",
    "max_value = max(nScore)\n",
    "min_value = min(nScore)\n",
    "inmin = nScore.index(min_value)\n",
    "inmax = nScore.index(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 : 0.18440511541192503\n",
      "46 : 0.10432861097730246\n",
      "58 : 0.10251566939396085\n",
      "73 : 0.10362749302278992\n",
      "83 : 0.34968416183086115\n",
      "95 : 0.12515464018114972\n"
     ]
    }
   ],
   "source": [
    "# look for average, below avg, etc.\n",
    "j = 0\n",
    "for i in nScore:\n",
    "    if 0.10 < i:\n",
    "        print(j,\":\",i)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I know why this place has bad reviews!Very unprofessional servers and the manager Kevin is an idiot.\n",
      "\n",
      "Andy - we'd like you to send us some more details about what happened during your visit so we can try to help. Please email Social@LongHornSteakhous… with your full name, receipt or order number, and phone number. Remind us you visited our Matthews location and that you originally contacted us on Yelp.\n",
      "\n",
      "we’re so sorry to hear about your experience you had at us at our center please let us know if you need any questions or concerns please feel free to let us know we can do more than exceptional we have been a solution to help you in any concerns and you can email us and thank you for reaching out of your day gratefully christian brothers coddle creek\n"
     ]
    }
   ],
   "source": [
    "# below average reply\n",
    "print(test_reviews[19])\n",
    "print()\n",
    "print(test_responses[19])\n",
    "print()\n",
    "print(translation[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market 168 is a good alternative to the busier Asian markets in the surrounding area. Staffing has always been helpful and accommodating.Pricing has been adjusted to be competitive with the other markets in town. Hot deli offers items not found at other market steam tables.\n",
      "\n",
      "Dear Sherpa O.,Thank you for taking the time to write this review. We appreciate your patronage, and hope to see you again soon!Sincerely,168 Market Customer Service Team\n",
      "\n",
      "thanks for your feedback and we really appreciate you taking the time to share your experience with us we appreciate your business\n"
     ]
    }
   ],
   "source": [
    "#average reply\n",
    "print(test_reviews[1])\n",
    "print()\n",
    "print(test_responses[1])\n",
    "print()\n",
    "print(translation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First to ReviewMy husband is military and the base we are stationed at recommended this place for printing. THEY ARE VETERAN OWNED AND OPERATED. Two nice guys work there and are very helpful. The prices are amazing, the place is clean and very organized. I will definitely be returning. So much closer and faster than the post officeOutside of We Got MailThe outside of We Got Mail\n",
      "\n",
      "thank you so much for your review. please come back in and take 15% off your total purchase. this is a thank you offer, and just remind me that i put this on: get any of the regular or brass keys made at .99 cents each (no limit!)!!!! just let us know that michael gave this offer!merry christmas!your friends from \"we got mail\"remeber all military get a 10% off thier total purchase at all times, and i will extend the .99 cent key offer to any military friends or family you refer!thanks again!\n",
      "\n",
      "john we are so glad you had the great experience with you we hope to see you again soon\n"
     ]
    }
   ],
   "source": [
    "#above average reply\n",
    "print(test_reviews[58])\n",
    "print()\n",
    "print(test_responses[58])\n",
    "print()\n",
    "print(translation[58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lot people hate on panda express because it's not real chinese food. that's true. but that's not why i'm complaining. they didn't give me enough shrimp when i ordered the walnut shrimp and they charged me an extra 1.50!i feel like you'll get much better and more food if you to a buffet like pacific seafood buffet.\n",
      "\n",
      "Hi John, thanks for bringing this to my attention. I'll check in with the restaurant team about this concern. Let me make it up to you. Please check your DM.\n",
      "\n",
      "thank you jerica\n"
     ]
    }
   ],
   "source": [
    "#max reply\n",
    "print(test_reviews[inmax])\n",
    "print()\n",
    "print(test_responses[inmax])\n",
    "print()\n",
    "print(translation[inmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las Vegas, NV11052719303/17/2021Rain is everything you want in a business. Friendly service, fast scheduling and very reasonable prices. Our water softener stopped working and I called a few places to come check it. Rain was the only company that would come out without a service charge. As expected our valve was cracked and the unit was not fixable. They gave me a quote for a replacement on a new system which was very fair. We decided to wait to replace it. I decided to have it removed and called a couple plumbers who quoted me very high prices to remove it and cap the water line. I called Rain again and they were happy to come out and do it for me. They gave me a very reasonable price and came out the next morning. The job was done quickly and the old unit removed. I couldn't be happier with the fast friendly service and pricing. When we decide to finally replace it I will definitely be calling Rain!\n",
      "\n",
      "Rain is everything you want in a business. Friendly service, fast scheduling and very reasonable prices. Our water softener stopped working and I called a few places to come check it. Rain was the only company that would come out without a service charge. As expected our valve was cracked and the unit was not fixable. They gave me a quote for a replacement on a new system which was very fair. We decided to wait to replace it. I decided to have it removed and called a couple plumbers who quoted me very high prices to remove it and cap the water line. I called Rain again and they were happy to come out and do it for me. They gave me a very reasonable price and came out the next morning. The job was done quickly and the old unit removed. I couldn't be happier with the fast friendly service and pricing. When we decide to finally replace it I will definitely be calling Rain!Useful 4Funny 1Cool 1Business owner informationMichael C.Business Owner3/17/2021Thank you for taking time to review our company!  We are very glad you are happy with the service received and we look forward to serving your future needs........Michael, The Rainman\n",
      "\n",
      "hello geraldinee p dr lamotte malone writing to be the home i never been to say the contract the baristas are very sweet but they even want to say the same foot the best i received i have never had the same caller and asked them a quick check and where it doesn't have the number to wear a phone call to call her so i called in my contract i told him my calendar was in the contract that i will be in so i have to wear the painful orthotic inserts that she's been prescribed in search of another pair of asics we ended up to see her there and agreed to you can email them so you can argue with the salesman again we were not happy with our services and do not understanding of your appointment they have never been on top of that plus no one of replacement was the same fee as the next morning he was very very much for me to find that you continued to deliver my service was so we can get the same world view as i would have to wear a secretary as a few minutes to get that you wanted to add coverage's to the judge of the future i would like this was the same as the contract that he didn't get the extra title from you are requested to get the wrong loan we received the original timing of the car is a few extra of the inspection that was 50 min could get me the car that is a clog that requires special attention  the pressure applied to the very nice job 3 amazing and not sure they could have a major roach returning the next morning my knees and so i was impressed to say this is the best for people since she was asked to the pharmacy for me to me that we have to have the best to fix it on the future it's always so we can continue to have to do our online appointment to rethink their appointment rooms because they want to be in the job i will be in so we have to have a job because of our same prices are all hour because of the body are not 24 hours notice as yelp is to mention that you could not to a conversation as this company i can be a different bra or underwear etc i have an appointment to sign the next time you could have a discount again i would have been helping the service was very reasonable they have their family back and next their next trip is to our family back in our choice is to our very very we are very much for a great experience although i will be pleased with the service team\n"
     ]
    }
   ],
   "source": [
    "#min reply\n",
    "print(test_reviews[inmin])\n",
    "print()\n",
    "print(test_responses[inmin])\n",
    "print()\n",
    "print(translation[inmin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
